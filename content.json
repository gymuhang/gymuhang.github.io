{"pages":[{"title":"Categories","date":"2020-04-22T06:40:46.907Z","path":"categories/index.html","text":""},{"title":"About","date":"2020-04-22T06:40:46.903Z","path":"about/index.html","text":""},{"title":"Tags","date":"2020-04-22T06:40:46.905Z","path":"tags/index.html","text":""}],"posts":[{"title":"Hello World","date":"2020-04-22T01:24:57.129Z","path":"wiki/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[],"categories":[]},{"title":"Seata 源码解析","date":"2020-04-22T01:24:57.129Z","path":"wiki/seata 源码/","text":"一款异步的事件驱动的网络应用程序框架 阻塞 发起 —-》阻塞…. 阻塞 —–》完成 检查…检查—-》阻塞—-》完成 ​ 阻塞与非阻塞是进程在访问数据的时候，数据是否准备就绪的处理方式，数据没准备好，要么等待数据准备好处理，要么直接返回即不处理 同步 双方的动作是经过双方协调的，步调一致的 异步 双方并不需要协调，都可以随意进行各自的操作 ​ 同步与异步都是基于应用程序和操作系统处理IO时间采用的方式，要么应用程序直接参与IO操作，要么IO操作交给操作系统去处理，应用程序只要等待通知 事件驱动：指在持续事务管理过程中，进行决策的一种策略，即跟随当前时间点上出现的事件，调动可用资源，执行相关任务，使不断出现的问题得以解决，防止事务堆积。（就是回调） bio：阻塞同步io nio：非阻塞io aio：非阻塞异步io ​ 为什么Netty使用NIO而不是AIO？ Netty不看重Windows上的使用，在Linux系统上，AIO的底层实现仍使用EPOLL，没有很好实现AIO，因此在性能上没有明显的优势，而且被JDK封装了一层不容易深度优化 Netty整体架构是reactor模型, 而AIO是proactor模型, 混合在一起会非常混乱,把AIO也改造成reactor模型看起来是把epoll绕个弯又绕回来 AIO还有个缺点是接收数据需要预先分配缓存, 而不是NIO那种需要接收时才需要分配缓存, 所以对连接数量非常大但流量小的情况, 内存浪费很多 Linux上AIO不够成熟，处理回调结果速度跟不到处理需求，比如外卖员太少，顾客太多，供不应求，造成处理速度有瓶颈（待验证） BIO是面向流的，一位置每次从流中读取字节，直至读取完全部字节，他们没有缓存在任何地方，因此是不能前后移动流中数据，需要移动或者操作的话需要将其缓存到缓冲区。 NIO是面向缓冲区的，数据读取到一个稍后处理的缓冲区，当然可以前后移动或者操作缓冲区数据。 1.组件1.Channel​ Java NIO的基本构造，代表一个到实体的开放连接，如读操作和写操作 2.回调​ 异步处理的后续操作 3.Future​ 提供了另一种操作完成时通知应用程序的方式，可以看作是异步操作结果的占位符，它在未来的某个时刻完成，并提供对其结果的访问，相对于jdk netty提供了自己的实现ChannelFuture 用的是ChannelFutureListener，即监听器 4.事件和ChannelHandler​ 事件就是 网络事件的出入站等，而ChannelHandler 则是对应具体事件的处理 5.放在一起​ Netty的异步编程模型建立在Future和回调上，并将事件派发到ChannelHandlerf方法。 ​ 触发事件 通过 抽象的Selector 进行派发代码，在内部为每个channel 分配EventLoop （线程驱动），来处理所有的事件 2.第一个应用3.netty的组件和设计1.Channel EventLoop ChannelFuture 网络抽象​ Channel 一个连接 socket （提供了很多默认实现） ​ EventLoop 控制流、多线程、并发 （相等于线程，EventLoop 相当于线程池）一个Channel 绑定一个 ​ EventLoop ，但是一个EventLoop 可能会分配给一个或多个Channel ​ ChannelFuture 异步通知 2.ChannelHandler ChannelPipeline 管理数据流以及执行应用程序处理逻辑​ ChannelHandler ​ ①入站和出站的数据的程序逻辑容器，②编解码，③异常通知，④channel编程活动或非活动的通知，⑤ ​ 注册Eventloop，或者注销 Eventloop的通知，⑥用户自定义的事件通知 ​ 常用的 ChannelHandlerAdapter ​ ChannelInboundHandlerAdapter ​ ChannelOutboundHandlerAdapter ​ ChannelDuplexHandler ​ 编码器、解码器 ​ SimpleChannelInboundHandler T 需要处理的数据类型 ctx是可以继续传递下去 ​ ChannelPipeline （拦截过滤器实现） ​ 是ChannelHandler 处理链的容器 出站和入站方向相反 ​ 引导 ​ 为应用程序网络层配置提供容器，将进程绑定和端口或者将进程连接到某个指定主机的指定端口进程，面 ​ 向连接的协议，如 tcp ​ Bootstrap 一个EventLoopGroup ​ ServerBootstrap 两个EventLoopGroup （当然也可以共用一个） 一个用来监听服务，一个用来处理传入 ​ 客户端的连接 4.传输​ OIO（阻塞）、NIO（非阻塞）、Local（JVM内部的异步通信）、Embedded（测试channelHandler） ​ netty的api 比较统一，只需要少量修改 1.传输的api​ channel 线程安全的 ​ EventLoop eventLoop(); 返回分配的eventLoop ​ ChannelPipeline pipeline(); 返回分配的ChannelPipeline ​ boolean isActive(); 是否是活动的 ​ SocketAddress localAddress(); 返回本地的 SocketAddress ​ SocketAddress remoteAddress(); 返回远程的 SocketAddress ​ ChannelFuture write(Object var1); 将数据写到远程节点，这个数据传递给 ChannelPipeline 写队列的第一个 ​ Channel flush(); 将之前写的数据 清空缓冲区数据 冲刷到 底层传输 ​ ChannelFuture writeAndFlush(Object var1); 写然后冲刷 2.内置的传输​ 提供的开箱可用的传输 ​ 1.NIO io.netty.channel.socket.nio java NIO为基础 ​ 选择器背后就是个注册表，当channel 发生变化时，得到通知，可能的变化： ​ 新的channel已经接受并且就绪 OP_ACCEPT ​ channel连接已经完成 OP_CONNECT ​ channel有已经就绪的可供读取的数据 OP_READ ​ channel可用于写数据 OP_WRITE ​ zero-copy（直接将数据从文件移动到网络接口） ​ 2.Epoll io.netty.channel.epoll 基于JNI驱动的epoll()和非阻塞IO，在linux上更快，比NIO更快 ​ linux jdk nio 使用了这一特性，但是netty 做了自己的统一封装 （使用了更加轻量的中断）比jdk 更高效 ​ 替换的话 Epoll 的serverSocket和EvevtLoopGroup ​ 3.OIO io.netty.channme.socket.oio 使用java.net 包的阻塞流 ​ 适用于某些阻塞的调用库（jdbc）等 ​ 4.Local io.netty.channel.local 在JVM内部通过管道进行通信的本地传输​ ​ 5.Embedded io.netty.channel.embedded Embedded传输，允许使用channelhandler不是真正的网络传输，为 ​ 了测试channelHandler 传输 TCP UDP SCT UDT NIO √ √ √ √ Epoll（仅linux） √ √ × × OIO √ √ √ √ 参考资料 nsetts","tags":[{"name":"seata","slug":"seata","permalink":"http://yoursite.com/tags/seata/"}],"categories":[{"name":"seata","slug":"seata","permalink":"http://yoursite.com/categories/seata/"}]},{"title":"netty源码解析","date":"2020-04-22T01:24:57.128Z","path":"wiki/netty/netty-action/","text":"一款异步的事件驱动的网络应用程序框架 阻塞 发起 —-》阻塞…. 阻塞 —–》完成 检查…检查—-》阻塞—-》完成 ​ 阻塞与非阻塞是进程在访问数据的时候，数据是否准备就绪的处理方式，数据没准备好，要么等待数据准备好处理，要么直接返回即不处理 同步 双方的动作是经过双方协调的，步调一致的 异步 双方并不需要协调，都可以随意进行各自的操作 ​ 同步与异步都是基于应用程序和操作系统处理IO时间采用的方式，要么应用程序直接参与IO操作，要么IO操作交给操作系统去处理，应用程序只要等待通知 事件驱动：指在持续事务管理过程中，进行决策的一种策略，即跟随当前时间点上出现的事件，调动可用资源，执行相关任务，使不断出现的问题得以解决，防止事务堆积。（就是回调） bio：阻塞同步io nio：非阻塞io aio：非阻塞异步io ​ 为什么Netty使用NIO而不是AIO？ Netty不看重Windows上的使用，在Linux系统上，AIO的底层实现仍使用EPOLL，没有很好实现AIO，因此在性能上没有明显的优势，而且被JDK封装了一层不容易深度优化 Netty整体架构是reactor模型, 而AIO是proactor模型, 混合在一起会非常混乱,把AIO也改造成reactor模型看起来是把epoll绕个弯又绕回来 AIO还有个缺点是接收数据需要预先分配缓存, 而不是NIO那种需要接收时才需要分配缓存, 所以对连接数量非常大但流量小的情况, 内存浪费很多 Linux上AIO不够成熟，处理回调结果速度跟不到处理需求，比如外卖员太少，顾客太多，供不应求，造成处理速度有瓶颈（待验证） BIO是面向流的，一位置每次从流中读取字节，直至读取完全部字节，他们没有缓存在任何地方，因此是不能前后移动流中数据，需要移动或者操作的话需要将其缓存到缓冲区。 NIO是面向缓冲区的，数据读取到一个稍后处理的缓冲区，当然可以前后移动或者操作缓冲区数据。 1.组件1.Channel​ Java NIO的基本构造，代表一个到实体的开放连接，如读操作和写操作 2.回调​ 异步处理的后续操作 3.Future​ 提供了另一种操作完成时通知应用程序的方式，可以看作是异步操作结果的占位符，它在未来的某个时刻完成，并提供对其结果的访问，相对于jdk netty提供了自己的实现ChannelFuture 用的是ChannelFutureListener，即监听器 4.事件和ChannelHandler​ 事件就是 网络事件的出入站等，而ChannelHandler 则是对应具体事件的处理 5.放在一起​ Netty的异步编程模型建立在Future和回调上，并将事件派发到ChannelHandlerf方法。 ​ 触发事件 通过 抽象的Selector 进行派发代码，在内部为每个channel 分配EventLoop （线程驱动），来处理所有的事件 2.第一个应用3.netty的组件和设计1.Channel EventLoop ChannelFuture 网络抽象​ Channel 一个连接 socket （提供了很多默认实现） ​ EventLoop 控制流、多线程、并发 （相等于线程，EventLoop 相当于线程池）一个Channel 绑定一个 ​ EventLoop ，但是一个EventLoop 可能会分配给一个或多个Channel ​ ChannelFuture 异步通知 2.ChannelHandler ChannelPipeline 管理数据流以及执行应用程序处理逻辑​ ChannelHandler ​ ①入站和出站的数据的程序逻辑容器，②编解码，③异常通知，④channel编程活动或非活动的通知，⑤ ​ 注册Eventloop，或者注销 Eventloop的通知，⑥用户自定义的事件通知 ​ 常用的 ChannelHandlerAdapter ​ ChannelInboundHandlerAdapter ​ ChannelOutboundHandlerAdapter ​ ChannelDuplexHandler ​ 编码器、解码器 ​ SimpleChannelInboundHandler T 需要处理的数据类型 ctx是可以继续传递下去 ​ ChannelPipeline （拦截过滤器实现） ​ 是ChannelHandler 处理链的容器 出站和入站方向相反 ​ 引导 ​ 为应用程序网络层配置提供容器，将进程绑定和端口或者将进程连接到某个指定主机的指定端口进程，面 ​ 向连接的协议，如 tcp ​ Bootstrap 一个EventLoopGroup ​ ServerBootstrap 两个EventLoopGroup （当然也可以共用一个） 一个用来监听服务，一个用来处理传入 ​ 客户端的连接 4.传输​ OIO（阻塞）、NIO（非阻塞）、Local（JVM内部的异步通信）、Embedded（测试channelHandler） ​ netty的api 比较统一，只需要少量修改 1.传输的api​ channel 线程安全的 ​ EventLoop eventLoop(); 返回分配的eventLoop ​ ChannelPipeline pipeline(); 返回分配的ChannelPipeline ​ boolean isActive(); 是否是活动的 ​ SocketAddress localAddress(); 返回本地的 SocketAddress ​ SocketAddress remoteAddress(); 返回远程的 SocketAddress ​ ChannelFuture write(Object var1); 将数据写到远程节点，这个数据传递给 ChannelPipeline 写队列的第一个 ​ Channel flush(); 将之前写的数据 清空缓冲区数据 冲刷到 底层传输 ​ ChannelFuture writeAndFlush(Object var1); 写然后冲刷 2.内置的传输​ 提供的开箱可用的传输 ​ 1.NIO io.netty.channel.socket.nio java NIO为基础 ​ 选择器背后就是个注册表，当channel 发生变化时，得到通知，可能的变化： ​ 新的channel已经接受并且就绪 OP_ACCEPT ​ channel连接已经完成 OP_CONNECT ​ channel有已经就绪的可供读取的数据 OP_READ ​ channel可用于写数据 OP_WRITE ​ zero-copy（直接将数据从文件移动到网络接口） ​ 2.Epoll io.netty.channel.epoll 基于JNI驱动的epoll()和非阻塞IO，在linux上更快，比NIO更快 ​ linux jdk nio 使用了这一特性，但是netty 做了自己的统一封装 （使用了更加轻量的中断）比jdk 更高效 ​ 替换的话 Epoll 的serverSocket和EvevtLoopGroup ​ 3.OIO io.netty.channme.socket.oio 使用java.net 包的阻塞流 ​ 适用于某些阻塞的调用库（jdbc）等 ​ 4.Local io.netty.channel.local 在JVM内部通过管道进行通信的本地传输​ ​ 5.Embedded io.netty.channel.embedded Embedded传输，允许使用channelhandler不是真正的网络传输，为 ​ 了测试channelHandler 传输 TCP UDP SCT UDT NIO √ √ √ √ Epoll（仅linux） √ √ × × OIO √ √ √ √","tags":[{"name":"netty","slug":"netty","permalink":"http://yoursite.com/tags/netty/"}],"categories":[{"name":"netty","slug":"netty","permalink":"http://yoursite.com/categories/netty/"}]},{"title":"Nacos 实战","date":"2020-04-22T01:24:57.128Z","path":"wiki/nacos/nacos/","text":"一款异步的事件驱动的网络应用程序框架 阻塞 发起 —-》阻塞…. 阻塞 —–》完成 检查…检查—-》阻塞—-》完成 ​ 阻塞与非阻塞是进程在访问数据的时候，数据是否准备就绪的处理方式，数据没准备好，要么等待数据准备好处理，要么直接返回即不处理 同步 双方的动作是经过双方协调的，步调一致的 异步 双方并不需要协调，都可以随意进行各自的操作 ​ 同步与异步都是基于应用程序和操作系统处理IO时间采用的方式，要么应用程序直接参与IO操作，要么IO操作交给操作系统去处理，应用程序只要等待通知 事件驱动：指在持续事务管理过程中，进行决策的一种策略，即跟随当前时间点上出现的事件，调动可用资源，执行相关任务，使不断出现的问题得以解决，防止事务堆积。（就是回调） bio：阻塞同步io nio：非阻塞io aio：非阻塞异步io ​ 为什么Netty使用NIO而不是AIO？ Netty不看重Windows上的使用，在Linux系统上，AIO的底层实现仍使用EPOLL，没有很好实现AIO，因此在性能上没有明显的优势，而且被JDK封装了一层不容易深度优化 Netty整体架构是reactor模型, 而AIO是proactor模型, 混合在一起会非常混乱,把AIO也改造成reactor模型看起来是把epoll绕个弯又绕回来 AIO还有个缺点是接收数据需要预先分配缓存, 而不是NIO那种需要接收时才需要分配缓存, 所以对连接数量非常大但流量小的情况, 内存浪费很多 Linux上AIO不够成熟，处理回调结果速度跟不到处理需求，比如外卖员太少，顾客太多，供不应求，造成处理速度有瓶颈（待验证） BIO是面向流的，一位置每次从流中读取字节，直至读取完全部字节，他们没有缓存在任何地方，因此是不能前后移动流中数据，需要移动或者操作的话需要将其缓存到缓冲区。 NIO是面向缓冲区的，数据读取到一个稍后处理的缓冲区，当然可以前后移动或者操作缓冲区数据。 1.组件1.Channel​ Java NIO的基本构造，代表一个到实体的开放连接，如读操作和写操作 2.回调​ 异步处理的后续操作 3.Future​ 提供了另一种操作完成时通知应用程序的方式，可以看作是异步操作结果的占位符，它在未来的某个时刻完成，并提供对其结果的访问，相对于jdk netty提供了自己的实现ChannelFuture 用的是ChannelFutureListener，即监听器 4.事件和ChannelHandler​ 事件就是 网络事件的出入站等，而ChannelHandler 则是对应具体事件的处理 5.放在一起​ Netty的异步编程模型建立在Future和回调上，并将事件派发到ChannelHandlerf方法。 ​ 触发事件 通过 抽象的Selector 进行派发代码，在内部为每个channel 分配EventLoop （线程驱动），来处理所有的事件 2.第一个应用3.netty的组件和设计1.Channel EventLoop ChannelFuture 网络抽象​ Channel 一个连接 socket （提供了很多默认实现） ​ EventLoop 控制流、多线程、并发 （相等于线程，EventLoop 相当于线程池）一个Channel 绑定一个 ​ EventLoop ，但是一个EventLoop 可能会分配给一个或多个Channel ​ ChannelFuture 异步通知 2.ChannelHandler ChannelPipeline 管理数据流以及执行应用程序处理逻辑​ ChannelHandler ​ ①入站和出站的数据的程序逻辑容器，②编解码，③异常通知，④channel编程活动或非活动的通知，⑤ ​ 注册Eventloop，或者注销 Eventloop的通知，⑥用户自定义的事件通知 ​ 常用的 ChannelHandlerAdapter ​ ChannelInboundHandlerAdapter ​ ChannelOutboundHandlerAdapter ​ ChannelDuplexHandler ​ 编码器、解码器 ​ SimpleChannelInboundHandler T 需要处理的数据类型 ctx是可以继续传递下去 ​ ChannelPipeline （拦截过滤器实现） ​ 是ChannelHandler 处理链的容器 出站和入站方向相反 ​ 引导 ​ 为应用程序网络层配置提供容器，将进程绑定和端口或者将进程连接到某个指定主机的指定端口进程，面 ​ 向连接的协议，如 tcp ​ Bootstrap 一个EventLoopGroup ​ ServerBootstrap 两个EventLoopGroup （当然也可以共用一个） 一个用来监听服务，一个用来处理传入 ​ 客户端的连接 4.传输​ OIO（阻塞）、NIO（非阻塞）、Local（JVM内部的异步通信）、Embedded（测试channelHandler） ​ netty的api 比较统一，只需要少量修改 1.传输的api​ channel 线程安全的 ​ EventLoop eventLoop(); 返回分配的eventLoop ​ ChannelPipeline pipeline(); 返回分配的ChannelPipeline ​ boolean isActive(); 是否是活动的 ​ SocketAddress localAddress(); 返回本地的 SocketAddress ​ SocketAddress remoteAddress(); 返回远程的 SocketAddress ​ ChannelFuture write(Object var1); 将数据写到远程节点，这个数据传递给 ChannelPipeline 写队列的第一个 ​ Channel flush(); 将之前写的数据 清空缓冲区数据 冲刷到 底层传输 ​ ChannelFuture writeAndFlush(Object var1); 写然后冲刷 2.内置的传输​ 提供的开箱可用的传输 ​ 1.NIO io.netty.channel.socket.nio java NIO为基础 ​ 选择器背后就是个注册表，当channel 发生变化时，得到通知，可能的变化： ​ 新的channel已经接受并且就绪 OP_ACCEPT ​ channel连接已经完成 OP_CONNECT ​ channel有已经就绪的可供读取的数据 OP_READ ​ channel可用于写数据 OP_WRITE ​ zero-copy（直接将数据从文件移动到网络接口） ​ 2.Epoll io.netty.channel.epoll 基于JNI驱动的epoll()和非阻塞IO，在linux上更快，比NIO更快 ​ linux jdk nio 使用了这一特性，但是netty 做了自己的统一封装 （使用了更加轻量的中断）比jdk 更高效 ​ 替换的话 Epoll 的serverSocket和EvevtLoopGroup ​ 3.OIO io.netty.channme.socket.oio 使用java.net 包的阻塞流 ​ 适用于某些阻塞的调用库（jdbc）等 ​ 4.Local io.netty.channel.local 在JVM内部通过管道进行通信的本地传输​ ​ 5.Embedded io.netty.channel.embedded Embedded传输，允许使用channelhandler不是真正的网络传输，为 ​ 了测试channelHandler 传输 TCP UDP SCT UDT NIO √ √ √ √ Epoll（仅linux） √ √ × × OIO √ √ √ √","tags":[],"categories":[{"name":"nacos","slug":"nacos","permalink":"http://yoursite.com/categories/nacos/"}]}]}